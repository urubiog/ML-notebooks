{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770c689e",
   "metadata": {
    "papermill": {
     "duration": 0.003381,
     "end_time": "2025-02-08T13:31:20.020796",
     "exception": false,
     "start_time": "2025-02-08T13:31:20.017415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "The MLP (Multi-Layer Perceptron) neural network model for handwritten digit prediction using the MNIST dataset in this example has a simple architecture, described below, along with its functionality.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The first input layer depends on the resolution of the images, which in this case are 28x28 (784 pixels). The second layer contains a total of 128 units, and finally, the last layer has 10 neurons corresponding to the possible digits in the decimal base (0-9).\n",
    "\n",
    "$$\n",
    "\\text{784 - 128 - 10}\n",
    "$$\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "For the interests of simplicity, we choose the ReLU (Rectified Linear Unit) activation function for the hidden layer and Sigmoid for the final layer. This is because the Sigmoid function transforms the network's output (which can have any real values) into a value between 0 and 1, allowing each output to be interpreted as the probability of the corresponding class in a binary classification task. However, unlike Softmax, Sigmoid does not ensure that the sum of all outputs equals 1. Instead, each output is treated independently.\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max{(0, z)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Sigmoid}(Z) = \\frac{e^{z_i}}{\\sum_{j=1}^K{e^{z_j}}}\n",
    "$$\n",
    "\n",
    "And, the derivatives for each function...\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d}z} \\text{ReLU}(z) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d}Z} \\text{Sigmoid}(Z) = \\text{Sigmoid}(Z) \\cdot (1 - \\text{Sigmoid}(Z))\n",
    "$$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "The Categorical Cross-Entropy (CE) function is used in classification problems, particularly when the output of the model is a probability distribution over multiple classes, as in the case of the softmax activation. In this case, the CE measures how well the predicted probability distribution matches the true class labels (0-9).\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log (\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_{ij}$ is the true label (1 if class $j$ is correct, 0 otherwise).\n",
    "- $\\hat{y}_{ij}$ is the predicted probability for class $j$ for example $i$.\n",
    "- $N$ is the number of training examples.\n",
    "- $K$ is the number of classes (for MNIST, $K = 10$).\n",
    "\n",
    "The derivative of the Categorical Cross-Entropy (CE) with respect to the predictions pre-activations is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_{i}} = \\frac{\\hat{y}_{i} - y_{i}}{N}\n",
    "$$\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "For the optimizer, we will use MGD (Mini-batch Gradient Descent) with a batch size $m$ of 128. Using the Backpropagation algorithm, we will reduce the loss by updating the parameters according to the differentiation of the previous functions, as shown in the adjacent formulas.\n",
    "\n",
    "The parameters (weights and biases) are updated as shown below. We introduce a new variable $\\delta$, which will be used to optimize the process and make it more manageable in terms of computational resources.\n",
    "\n",
    "$$\n",
    "\\delta^{L}_i = \\frac{\\partial{\\mathcal{L}}}{\\partial{z^L_i}} = \\frac{\\partial{\\mathcal{L}}}{\\partial{\\hat{y}^L_i}} · \\frac{\\partial{\\hat{y}^L_i}}{\\partial{z^L_i}}\n",
    "$$\n",
    "\n",
    "For the last layer $L$.\n",
    "\n",
    "$$\n",
    "\\delta^{(l-1)}_i = \\frac{\\partial{\\mathcal{L}}}{\\partial{z^{(l-1)}_i}} = \\left(\\sum_{j}{\\delta^{(l)}_j · w^{(l)}_{ij}}\\right) · \\frac{\\partial{a^{(l-1)}_i}}{\\partial{z^{(l-1)}_i}}\n",
    "$$\n",
    "\n",
    "For the layer preceding to $l$.\n",
    "\n",
    "$$\n",
    "b^{(l)}_i := b^{(l)}_i - \\alpha \\frac{\\partial{\\mathcal{L}}}{\\partial{b^{(l)}_{i}}} = \\delta^{(l)}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(l)}_{ij} := w^{(l)}_{ij} - \\alpha \\frac{\\partial{\\mathcal{L}}}{\\partial{w^{(l)}_{ij}}} = \\delta^{(l)}_i · a^{(l-1)}_j\n",
    "$$\n",
    "\n",
    "For each iteration.\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\frac{1}{m} \\sum^{m}_{i=1}{\\nabla_{\\theta} \\mathcal{L}(\\theta;x^{(i)}, y^{(i)})}\n",
    "$$\n",
    "\n",
    "For each batch.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Regarding hyperparameters, we will use standardized values:\n",
    "\n",
    "$$\n",
    "\\eta: \\text{learning rate} = 0.01\n",
    "$$\n",
    "\n",
    "$$\n",
    "m: \\text{batch size} = 128\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3dd3eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:31:20.028014Z",
     "iopub.status.busy": "2025-02-08T13:31:20.027655Z",
     "iopub.status.idle": "2025-02-08T13:31:20.950031Z",
     "shell.execute_reply": "2025-02-08T13:31:20.949127Z"
    },
    "papermill": {
     "duration": 0.928055,
     "end_time": "2025-02-08T13:31:20.952003",
     "exception": false,
     "start_time": "2025-02-08T13:31:20.023948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f844014e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:31:20.959166Z",
     "iopub.status.busy": "2025-02-08T13:31:20.958673Z",
     "iopub.status.idle": "2025-02-08T13:31:28.267049Z",
     "shell.execute_reply": "2025-02-08T13:31:28.266040Z"
    },
    "papermill": {
     "duration": 7.313673,
     "end_time": "2025-02-08T13:31:28.268805",
     "exception": false,
     "start_time": "2025-02-08T13:31:20.955132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.00000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.456643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219286</td>\n",
       "      <td>0.117095</td>\n",
       "      <td>0.059024</td>\n",
       "      <td>0.02019</td>\n",
       "      <td>0.017238</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.887730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.312890</td>\n",
       "      <td>4.633819</td>\n",
       "      <td>3.274488</td>\n",
       "      <td>1.75987</td>\n",
       "      <td>1.894498</td>\n",
       "      <td>0.414264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.00000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\n",
       "count  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \n",
       "mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\n",
       "count  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \n",
       "mean       0.0      0.0      0.0  ...      0.219286      0.117095   \n",
       "std        0.0      0.0      0.0  ...      6.312890      4.633819   \n",
       "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
       "\n",
       "           pixel776     pixel777      pixel778      pixel779  pixel780  \\\n",
       "count  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \n",
       "mean       0.059024      0.02019      0.017238      0.002857       0.0   \n",
       "std        3.274488      1.75987      1.894498      0.414264       0.0   \n",
       "min        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "25%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "50%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "75%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "max      253.000000    253.00000    254.000000     62.000000       0.0   \n",
       "\n",
       "       pixel781  pixel782  pixel783  \n",
       "count   42000.0   42000.0   42000.0  \n",
       "mean        0.0       0.0       0.0  \n",
       "std         0.0       0.0       0.0  \n",
       "min         0.0       0.0       0.0  \n",
       "25%         0.0       0.0       0.0  \n",
       "50%         0.0       0.0       0.0  \n",
       "75%         0.0       0.0       0.0  \n",
       "max         0.0       0.0       0.0  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "train_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\n",
    "test_data = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\n",
    "\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d295d42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:31:28.277856Z",
     "iopub.status.busy": "2025-02-08T13:31:28.277531Z",
     "iopub.status.idle": "2025-02-08T13:31:28.466120Z",
     "shell.execute_reply": "2025-02-08T13:31:28.465143Z"
    },
    "papermill": {
     "duration": 0.194464,
     "end_time": "2025-02-08T13:31:28.467916",
     "exception": false,
     "start_time": "2025-02-08T13:31:28.273452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 40000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data division\n",
    "data = np.array(train_data) # 42_000 examples\n",
    "test_data = np.array(test_data)   # 28_000 examples (without labels)\n",
    "\n",
    "m, n = train_data.shape\n",
    "\n",
    "train_data = data[2000:m].T\n",
    "X_train = train_data[1: n]\n",
    "Y_train = train_data[0]\n",
    "\n",
    "eval_data = data[:2000].T\n",
    "X_eval = eval_data[1: n]\n",
    "Y_eval = eval_data[0]\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c25a61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:31:28.476465Z",
     "iopub.status.busy": "2025-02-08T13:31:28.476087Z",
     "iopub.status.idle": "2025-02-08T13:31:28.657988Z",
     "shell.execute_reply": "2025-02-08T13:31:28.657014Z"
    },
    "papermill": {
     "duration": 0.188111,
     "end_time": "2025-02-08T13:31:28.659723",
     "exception": false,
     "start_time": "2025-02-08T13:31:28.471612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 784), (40000, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "test_data = test_data.reshape(28000, 28, 28)\n",
    "\n",
    "X_train = X_train.astype(np.float64).T\n",
    "X_eval = X_eval.astype(np.float64).T\n",
    "\n",
    "max_val = X_train.max() # 255.0\n",
    "\n",
    "X_train /= max_val # Normalization\n",
    "X_eval /= max_val\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    encoded_Y = np.zeros((Y.size, Y.max()+1), dtype=int)\n",
    "    encoded_Y[np.arange(Y.size), Y] = 1 \n",
    "    return encoded_Y\n",
    "\n",
    "Y_train_encoded = one_hot_encode(Y_train)\n",
    "Y_eval_encoded = one_hot_encode(Y_eval)\n",
    "\n",
    "X_train.shape, Y_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24126113",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-02-08T13:31:28.667948Z",
     "iopub.status.busy": "2025-02-08T13:31:28.667621Z",
     "iopub.status.idle": "2025-02-08T13:31:28.693539Z",
     "shell.execute_reply": "2025-02-08T13:31:28.692428Z"
    },
    "papermill": {
     "duration": 0.032305,
     "end_time": "2025-02-08T13:31:28.695582",
     "exception": false,
     "start_time": "2025-02-08T13:31:28.663277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class MLP:\n",
    "    def __init__(self, hidden_layer_units = 128):\n",
    "        self.p = 28*28\n",
    "        self.h = hidden_layer_units\n",
    "        self.q = 10\n",
    "        \n",
    "        self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        # [-0.5, 0.5) distribution\n",
    "        self.W1 = np.random.rand(self.h, self.p) - 0.5\n",
    "        self.b1 = np.random.rand(self.h) - 0.5\n",
    "        self.W2 = np.random.rand(self.q, self.h) - 0.5\n",
    "        self.b2 = np.random.rand(self.q) - 0.5\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_deriv(z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        z = np.clip(z, -500, 500) # Avoid possible errors\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_deriv(z):\n",
    "        s = MLP.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(pred, true):  # Categorical Cross-Entropy (CE)\n",
    "        return -np.sum(true * np.log(pred + 1e-8)) / true.shape[0]  # avoid log(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_deriv(pred, true):\n",
    "        return (pred - true) / true.shape[0]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.Z1 = self.W1 @ x + self.b1\n",
    "        self.A1 = MLP.relu(self.Z1)\n",
    "        self.Z2 = self.W2 @ self.A1 + self.b2\n",
    "        self.A2 = MLP.sigmoid(self.Z2)\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        # deltas\n",
    "        dZ2 = MLP.loss_deriv(self.A2, y)\n",
    "        dZ1 = self.W2.T @ dZ2 * MLP.relu_deriv(self.Z1)\n",
    "\n",
    "        # gradients\n",
    "        dW2 = np.outer(dZ2, self.A1)\n",
    "        db2 = dZ2\n",
    "        dW1 = np.outer(dZ1, x)\n",
    "        db1 = dZ1\n",
    "        \n",
    "        return dW2, db2, dW1, db1      \n",
    "\n",
    "    def update_parameters(self, learning_rate, dW2_avg, db2_avg, dW1_avg, db1_avg):\n",
    "        self.W2 -= learning_rate * dW2_avg\n",
    "        self.b2 -= learning_rate * db2_avg\n",
    "        self.W1 -= learning_rate * dW1_avg\n",
    "        self.b1 -= learning_rate * db1_avg\n",
    "    \n",
    "    def train(self, X, Y, epochs = 10, learning_rate = 1e-2, batch_size = 128):\n",
    "        n = Y.shape[0]\n",
    "        batches = n // batch_size # Last incomplete batch won't be utilized\n",
    "        \n",
    "        # MGD\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch in range(batches):\n",
    "                grads_avg = list(map(np.zeros_like, (self.W2, self.b2, self.W1, self.b1)))\n",
    "                \n",
    "                for i in range(batch * batch_size, (batch+1) * batch_size):\n",
    "                    xi = X[i]\n",
    "                    yi = Y[i]\n",
    "\n",
    "                    self.forward(xi)\n",
    "                    loss = MLP.loss(self.A2, yi)\n",
    "                    epoch_loss += loss\n",
    "                    \n",
    "                    grads = self.backward(xi, yi)\n",
    "\n",
    "                    for grad, grad_avg in zip(grads, grads_avg):\n",
    "                        grad_avg += grad\n",
    "\n",
    "                for grad_avg in grads:\n",
    "                    grad_avg /= batch_size\n",
    "                    \n",
    "                self.update_parameters(learning_rate, *grads_avg)\n",
    "                \n",
    "            avg_epoch_loss = epoch_loss / (batches * batch_size)\n",
    "            print(f\"Epoch [{epoch + 1}] - Loss: {avg_epoch_loss}\")\n",
    "            \n",
    "    def evaluate(self, X, Y):\n",
    "        samples = X.shape[0]\n",
    "        correct = 0\n",
    "        for i in range(samples):\n",
    "            xi = X[i]\n",
    "            yi = Y[i]\n",
    "            self.forward(xi)\n",
    "            pred_class = np.argmax(self.A2)\n",
    "            true_class = np.argmax(yi)\n",
    "            if pred_class == true_class:\n",
    "                correct += 1\n",
    "        \n",
    "        accuracy = correct / X.shape[0]\n",
    "        print(f\"Samples: {samples} - Accuracy: {accuracy}\")     \n",
    "\n",
    "# Instatiate the model\n",
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a2c2d06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:31:28.704029Z",
     "iopub.status.busy": "2025-02-08T13:31:28.703651Z",
     "iopub.status.idle": "2025-02-08T13:34:13.157850Z",
     "shell.execute_reply": "2025-02-08T13:34:13.156741Z"
    },
    "papermill": {
     "duration": 164.464143,
     "end_time": "2025-02-08T13:34:13.163527",
     "exception": false,
     "start_time": "2025-02-08T13:31:28.699384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] - Loss: 0.07518985325080542\n",
      "Epoch [2] - Loss: 0.03705287341792626\n",
      "Epoch [3] - Loss: 0.03012161021228379\n",
      "Epoch [4] - Loss: 0.026141105705808416\n",
      "Epoch [5] - Loss: 0.023404396496873228\n",
      "Epoch [6] - Loss: 0.02134932241281277\n",
      "Epoch [7] - Loss: 0.019719979891590692\n",
      "Epoch [8] - Loss: 0.018379721686729383\n",
      "Epoch [9] - Loss: 0.01724696039438065\n",
      "Epoch [10] - Loss: 0.016264616489841954\n",
      "Samples: 2000 - Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Training and Evaluation\n",
    "mlp.train(X_train, Y_train_encoded)\n",
    "mlp.evaluate(X_eval, Y_eval_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a086408e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T13:34:13.172855Z",
     "iopub.status.busy": "2025-02-08T13:34:13.172533Z",
     "iopub.status.idle": "2025-02-08T13:34:13.484252Z",
     "shell.execute_reply": "2025-02-08T13:34:13.483114Z"
    },
    "papermill": {
     "duration": 0.318507,
     "end_time": "2025-02-08T13:34:13.486172",
     "exception": false,
     "start_time": "2025-02-08T13:34:13.167665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests\n",
    "sample = np.random.randint(1, test_data.shape[0])\n",
    "num = test_data[sample]\n",
    "\n",
    "plt.title(f\"Sample {sample}\")\n",
    "plt.imshow(num, cmap=\"gray\")\n",
    "mlp.forward(num.reshape(28*28))\n",
    "\n",
    "print(f\"Model prediction: {mlp.A2.argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b5b51",
   "metadata": {
    "papermill": {
     "duration": 0.004563,
     "end_time": "2025-02-08T13:34:13.495544",
     "exception": false,
     "start_time": "2025-02-08T13:34:13.490981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">Thanks for reading, \\\n",
    "> Uriel Rubio García | @[urubiog](https://www.github.com/urubiog)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 176.912659,
   "end_time": "2025-02-08T13:34:14.120839",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-08T13:31:17.208180",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
