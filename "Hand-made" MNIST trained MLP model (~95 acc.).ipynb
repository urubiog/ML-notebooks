{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f2cd34",
   "metadata": {
    "papermill": {
     "duration": 0.003502,
     "end_time": "2025-02-02T22:19:51.816051",
     "exception": false,
     "start_time": "2025-02-02T22:19:51.812549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "The MLP (Multi-Layer Perceptron) neural network model for handwritten digit prediction using the MNIST dataset in this example has a simple architecture, described below, along with its functionality.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The first input layer depends on the resolution of the images, which in this case are 28x28 (784 pixels). The second layer contains a total of 128 units, and finally, the last layer has 10 neurons corresponding to the possible digits in the decimal base (0-9).\n",
    "\n",
    "$$\n",
    "\\text{784 - 128 - 10}\n",
    "$$\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "For simplicity, we choose the ReLU (Rectified Linear Unit) activation function for the hidden layer and Sigmoid for the final layer. This is because the Sigmoid function transforms the network's output (which can have any real values) into a value between 0 and 1, allowing each output to be interpreted as the probability of the corresponding class in a binary classification task. However, unlike Softmax, Sigmoid does not ensure that the sum of all outputs equals 1. Instead, each output is treated independently.\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max{(0, z)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Sigmoid}(Z) = \\frac{e^{z_i}}{\\sum_{j=1}^K{e^{z_j}}}\n",
    "$$\n",
    "\n",
    "And, the derivatives for each function...\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d}z} \\text{ReLU}(z) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d}Z} \\text{Sigmoid}(Z) = \\text{Sigmoid}(Z) \\cdot (1 - \\text{Sigmoid}(Z))\n",
    "$$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "The Categorical Cross-Entropy (CE) function is used in classification problems, particularly when the output of the model is a probability distribution over multiple classes, as in the case of the softmax activation. In this case, the CE measures how well the predicted probability distribution matches the true class labels (0-9).\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log (\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_{ij}$ is the true label (1 if class $j$ is correct, 0 otherwise).\n",
    "- $\\hat{y}_{ij}$ is the predicted probability for class $j$ for example $i$.\n",
    "- $N$ is the number of training examples.\n",
    "- $K$ is the number of classes (for MNIST, $K = 10$).\n",
    "\n",
    "The derivative of the Categorical Cross-Entropy (CE) with respect to the predictions is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_{ij}} = \\frac{\\hat{y}_{ij} - y_{ij}}{N}\n",
    "$$\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "For the optimizer, we will use MGD (Mini-batch Gradient Descent) with a batch size $m$ of 128. Using the Backpropagation algorithm, we will reduce the loss by updating the parameters according to the differentiation of the previous functions, as shown in the adjacent formulas.\n",
    "\n",
    "The parameters (weights and biases) are updated as shown below. We introduce a new variable $\\delta$, which will be used to optimize the process and make it more manageable in terms of computational resources.\n",
    "\n",
    "$$\n",
    "\\delta^{L}_i = \\frac{\\partial{\\mathcal{L}}}{\\partial{z^L_i}} = \\frac{\\partial{\\mathcal{L}}}{\\partial{\\hat{y}^L_i}} · \\frac{\\partial{\\hat{y}^L_i}}{\\partial{z^L_i}}\n",
    "$$\n",
    "\n",
    "For the last layer $L$.\n",
    "\n",
    "$$\n",
    "\\delta^{(l-1)}_i = \\frac{\\partial{\\mathcal{L}}}{\\partial{z^{(l-1)}_i}} = \\left(\\sum_{j}{\\delta^{(l)}_j · w^{l}_{ij}}\\right) · \\frac{\\partial{a^{(l-1)}_i}}{\\partial{z^{(l-1)}_i}}\n",
    "$$\n",
    "\n",
    "For the layer preceding to $l$.\n",
    "\n",
    "$$\n",
    "b^{(l)}_i := b^{(l)}_i - \\alpha \\frac{\\partial{\\mathcal{L}}}{\\partial{b^{(l)}_{i}}} = \\delta^{(l)}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(l)}_{ij} := w^{(l)}_{ij} - \\alpha \\frac{\\partial{\\mathcal{L}}}{\\partial{w^{(l)}_{ij}}} = \\delta^{(l)}_i · a^{(l-1)}_j\n",
    "$$\n",
    "\n",
    "For each iteration.\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\frac{1}{m} \\sum^{m}_{i=1}{\\nabla_{\\theta} \\mathcal{L}(\\theta;x^{(i)}, y^{(i)})}\n",
    "$$\n",
    "\n",
    "For each batch.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Regarding hyperparameters, we will use standardized values:\n",
    "\n",
    "$$\n",
    "\\eta: \\text{learning rate} = 0.01\n",
    "$$\n",
    "\n",
    "$$\n",
    "m: \\text{batch size} = 128\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49f782f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T22:19:51.823384Z",
     "iopub.status.busy": "2025-02-02T22:19:51.822955Z",
     "iopub.status.idle": "2025-02-02T22:19:52.870016Z",
     "shell.execute_reply": "2025-02-02T22:19:52.868512Z"
    },
    "papermill": {
     "duration": 1.053039,
     "end_time": "2025-02-02T22:19:52.872117",
     "exception": false,
     "start_time": "2025-02-02T22:19:51.819078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b762a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T22:19:52.879666Z",
     "iopub.status.busy": "2025-02-02T22:19:52.879047Z",
     "iopub.status.idle": "2025-02-02T22:20:00.789644Z",
     "shell.execute_reply": "2025-02-02T22:20:00.788226Z"
    },
    "papermill": {
     "duration": 7.916201,
     "end_time": "2025-02-02T22:20:00.791427",
     "exception": false,
     "start_time": "2025-02-02T22:19:52.875226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.00000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>42000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.456643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219286</td>\n",
       "      <td>0.117095</td>\n",
       "      <td>0.059024</td>\n",
       "      <td>0.02019</td>\n",
       "      <td>0.017238</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.887730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.312890</td>\n",
       "      <td>4.633819</td>\n",
       "      <td>3.274488</td>\n",
       "      <td>1.75987</td>\n",
       "      <td>1.894498</td>\n",
       "      <td>0.414264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.00000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\n",
       "count  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \n",
       "mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\n",
       "count  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \n",
       "mean       0.0      0.0      0.0  ...      0.219286      0.117095   \n",
       "std        0.0      0.0      0.0  ...      6.312890      4.633819   \n",
       "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
       "\n",
       "           pixel776     pixel777      pixel778      pixel779  pixel780  \\\n",
       "count  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \n",
       "mean       0.059024      0.02019      0.017238      0.002857       0.0   \n",
       "std        3.274488      1.75987      1.894498      0.414264       0.0   \n",
       "min        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "25%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "50%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "75%        0.000000      0.00000      0.000000      0.000000       0.0   \n",
       "max      253.000000    253.00000    254.000000     62.000000       0.0   \n",
       "\n",
       "       pixel781  pixel782  pixel783  \n",
       "count   42000.0   42000.0   42000.0  \n",
       "mean        0.0       0.0       0.0  \n",
       "std         0.0       0.0       0.0  \n",
       "min         0.0       0.0       0.0  \n",
       "25%         0.0       0.0       0.0  \n",
       "50%         0.0       0.0       0.0  \n",
       "75%         0.0       0.0       0.0  \n",
       "max         0.0       0.0       0.0  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "train_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\n",
    "test_data = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\n",
    "\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc379b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T22:20:00.801665Z",
     "iopub.status.busy": "2025-02-02T22:20:00.801295Z",
     "iopub.status.idle": "2025-02-02T22:20:00.995418Z",
     "shell.execute_reply": "2025-02-02T22:20:00.994167Z"
    },
    "papermill": {
     "duration": 0.200405,
     "end_time": "2025-02-02T22:20:00.997213",
     "exception": false,
     "start_time": "2025-02-02T22:20:00.796808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 40000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data division\n",
    "data = np.array(train_data) # 42_000 examples\n",
    "test_data = np.array(test_data)   # 28_000 examples (without labels)\n",
    "\n",
    "m, n = train_data.shape\n",
    "\n",
    "train_data = data[2000:m].T\n",
    "X_train = train_data[1: n]\n",
    "Y_train = train_data[0]\n",
    "\n",
    "eval_data = data[:2000].T\n",
    "X_eval = eval_data[1: n]\n",
    "Y_eval = eval_data[0]\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b44e3e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T22:20:01.005102Z",
     "iopub.status.busy": "2025-02-02T22:20:01.004804Z",
     "iopub.status.idle": "2025-02-02T22:20:01.149092Z",
     "shell.execute_reply": "2025-02-02T22:20:01.148116Z"
    },
    "papermill": {
     "duration": 0.15008,
     "end_time": "2025-02-02T22:20:01.150815",
     "exception": false,
     "start_time": "2025-02-02T22:20:01.000735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 784), (40000, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "X_train = X_train.astype(np.float64).T\n",
    "X_train /= 255.0 # Normalization\n",
    "X_eval = X_eval.astype(np.float64).T\n",
    "X_eval /= 255.0\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    encoded_Y = np.zeros((Y.size, Y.max()+1), dtype=int)\n",
    "    encoded_Y[np.arange(Y.size), Y] = 1 \n",
    "    return encoded_Y\n",
    "\n",
    "Y_train_encoded = one_hot_encode(Y_train)\n",
    "Y_eval_encoded = one_hot_encode(Y_eval)\n",
    "\n",
    "X_train.shape, Y_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cbd2193",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-02-02T22:20:01.158836Z",
     "iopub.status.busy": "2025-02-02T22:20:01.158510Z",
     "iopub.status.idle": "2025-02-02T22:20:01.182925Z",
     "shell.execute_reply": "2025-02-02T22:20:01.181911Z"
    },
    "papermill": {
     "duration": 0.030745,
     "end_time": "2025-02-02T22:20:01.185041",
     "exception": false,
     "start_time": "2025-02-02T22:20:01.154296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class MLP:\n",
    "    def __init__(self, hidden_layer_units = 128):\n",
    "        self.p = 28*28\n",
    "        self.h = hidden_layer_units\n",
    "        self.q = 10\n",
    "        \n",
    "        self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        self.W1 = np.random.rand(self.h, self.p) - 0.5\n",
    "        self.b1 = np.random.rand(self.h) - 0.5\n",
    "        self.W2 = np.random.rand(self.q, self.h) - 0.5\n",
    "        self.b2 = np.random.rand(self.q) - 0.5\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_deriv(z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        z = np.clip(z, -500, 500) # Avoid possible errors\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_deriv(z):\n",
    "        s = MLP.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(pred, true):  # Categorical Cross-Entropy (CE)\n",
    "        return -np.sum(true * np.log(pred + 1e-8)) / true.shape[0]  # avoid log(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_deriv(pred, true):\n",
    "        return (pred - true) / true.shape[0]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.Z1 = self.W1.dot(x) + self.b1\n",
    "        self.A1 = MLP.relu(self.Z1)\n",
    "        self.Z2 = self.W2.dot(self.A1) + self.b2\n",
    "        self.A2 = MLP.sigmoid(self.Z2)\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        # deltas\n",
    "        dZ2 = self.A2 - y\n",
    "        dZ1 = self.W2.T.dot(dZ2) * MLP.relu_deriv(self.Z1)\n",
    "\n",
    "        # gradients\n",
    "        dW2 = np.outer(dZ2, self.A1)\n",
    "        db2 = dZ2\n",
    "        dW1 = np.outer(dZ1, x)\n",
    "        db1 = dZ1\n",
    "        \n",
    "        return dW2, db2, dW1, db1      \n",
    "\n",
    "    def update_parameters(self, learning_rate, dW2, db2, dW1, db1):\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def train(self, X, Y, epochs = 10, learning_rate = 1e-2, batch_size = 128):\n",
    "        n = Y.shape[0]\n",
    "        batches = n // batch_size\n",
    "        \n",
    "        # MGD\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = -1\n",
    "            \n",
    "            for batch in range(batches):\n",
    "                grads_avg = list(map(np.zeros_like, (self.W2, self.b2, self.W1, self.b1)))\n",
    "                \n",
    "                for i in range(batch * batch_size, (batch+1) * batch_size):\n",
    "                    xi = X[i]\n",
    "                    yi = Y[i]\n",
    "\n",
    "                    self.forward(xi)\n",
    "                    loss = MLP.loss(self.A2, yi)\n",
    "                    epoch_loss += loss\n",
    "                    \n",
    "                    grads = self.backward(xi, yi)\n",
    "\n",
    "                    for grad, grad_avg in zip(grads, grads_avg):\n",
    "                        grad_avg += grad\n",
    "\n",
    "                for grad_avg in grads:\n",
    "                    grad_avg /= batch_size\n",
    "                    \n",
    "                self.update_parameters(learning_rate, *grads_avg)\n",
    "                \n",
    "            avg_epoch_loss = epoch_loss / (batches * batch_size)\n",
    "            print(f\"Epoch [{epoch + 1}] - Loss: {avg_epoch_loss}\")\n",
    "            \n",
    "    def evaluate(self, X, Y):\n",
    "        samples = X.shape[0]\n",
    "        correct = 0\n",
    "        for i in range(samples):\n",
    "            xi = X[i]\n",
    "            yi = Y[i]\n",
    "            self.forward(xi)\n",
    "            pred_class = np.argmax(self.A2)\n",
    "            true_class = np.argmax(yi)\n",
    "            if pred_class == true_class:\n",
    "                correct += 1\n",
    "        print(i)\n",
    "        \n",
    "        accuracy = correct / X.shape[0]\n",
    "        print(f\"Samples: {samples} - Accuracy: {accuracy}\")     \n",
    "\n",
    "# Instatiate the model\n",
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb4673d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T22:20:01.195073Z",
     "iopub.status.busy": "2025-02-02T22:20:01.194735Z",
     "iopub.status.idle": "2025-02-02T22:22:43.215226Z",
     "shell.execute_reply": "2025-02-02T22:22:43.214116Z"
    },
    "papermill": {
     "duration": 162.029231,
     "end_time": "2025-02-02T22:22:43.218807",
     "exception": false,
     "start_time": "2025-02-02T22:20:01.189576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] - Loss: 0.06824091030775992\n",
      "Epoch [2] - Loss: 0.03048615630057137\n",
      "Epoch [3] - Loss: 0.026792251963874802\n",
      "Epoch [4] - Loss: 0.024407599447696122\n",
      "Epoch [5] - Loss: 0.02266707217135463\n",
      "Epoch [6] - Loss: 0.021324600899608775\n",
      "Epoch [7] - Loss: 0.019997204897106984\n",
      "Epoch [8] - Loss: 0.018826030636812246\n",
      "Epoch [9] - Loss: 0.017813518260651272\n",
      "Epoch [10] - Loss: 0.016942023133278222\n",
      "1999\n",
      "Samples: 2000 - Accuracy: 0.9445\n"
     ]
    }
   ],
   "source": [
    "# Training and Evaluation\n",
    "mlp.train(X_train, Y_train_encoded)\n",
    "mlp.evaluate(X_eval, Y_eval_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a116e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T22:22:43.227995Z",
     "iopub.status.busy": "2025-02-02T22:22:43.227659Z",
     "iopub.status.idle": "2025-02-02T22:22:43.231537Z",
     "shell.execute_reply": "2025-02-02T22:22:43.230419Z"
    },
    "papermill": {
     "duration": 0.010303,
     "end_time": "2025-02-02T22:22:43.233206",
     "exception": false,
     "start_time": "2025-02-02T22:22:43.222903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tests\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd18e7d",
   "metadata": {
    "papermill": {
     "duration": 0.003699,
     "end_time": "2025-02-02T22:22:43.241169",
     "exception": false,
     "start_time": "2025-02-02T22:22:43.237470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">Thanks for reading, \\\n",
    "> Uriel Rubio García | @[urubiog](https://www.github.com/urubiog)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 174.979894,
   "end_time": "2025-02-02T22:22:43.865537",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-02T22:19:48.885643",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
